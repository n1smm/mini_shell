lexer espande le variabili d'ambiente (expander) 
poi spezza la linea dell'input in base alle pipe
cosi sapete gia quanti comandi ci sono

garbage collector 
ogni indirizzo allocato viene aggiunto ad una lista 
alla fine del programma viene tutto liberato
(va chiamato sia alla fine di minishell che alla 
fine dei processi figlio)

singleton


parser
divide i comandi in -commando -args -input -output
(tieni conto delle virgolette)

executor
gli passa i comandi e li esegue
fai attenzione. se un commando non e' built in va 
fatto in un altro processo altrimenti si chiude minishell

(es. cd cartella | pwd ...)
non occorre gestire le parentesi graffe con l'espander

readline


Lexical analysis / tokenization: taking the input from the user and processing it char by char into “tokens”.

Syntax analysis / Parsing: scanning the stream of tokens according to a specific grammar and then deciding what to do with them (e.g. generating an AST — Abstract Syntax Tree).
!!!Abstract Syntax Tree).


 The Recursive Descent - The Theory of Computation,

  CFG (Context Free Grammar).
  
  for parsing we need to do:

1. tokenization (lexical analysis) 
	here we transform the input line into tokens
		which means that we split the input by whitespaces : (ls -l /home/user) becomes
																- ls
																- l
																- /home/user
2. syntax analysis (parsing)
	-we generate an abstract syntax tree (AST) which means that the program tries to understand what to do with 	tokes



3. executor
- needs to handle multiple redirects correctly (also if only redirect is given)
- command needs to be handled correctly ( so passing what comes from parser and transforming it into a formatted string or something similar
- if before pipe there is redirect, than the std out needs to be empty.

- need to change a bit the parsing part . for example command can also be after OUT/INFILE or heredoc ...etc. possible other changes aswell

- so we need to know the order of redirections, and where do we pass the command to
- 
