lexer espande le variabili d'ambiente (expander) 
poi spezza la linea dell'input in base alle pipe
cosi sapete gia quanti comandi ci sono

garbage collector 
ogni indirizzo allocato viene aggiunto ad una lista 
alla fine del programma viene tutto liberato
(va chiamato sia alla fine di minishell che alla 
fine dei processi figlio)

singleton


parser
divide i comandi in -comando -args -input -output
(tieni conto delle virgolette)

executor
gli passa i comandi e li esegue
fai attenzione. se un comando non e' built in va 
fatto in un altro processo altrimenti si chiude minishell

(es. cd cartella | pwd ...)
non occorre gestire le parentesi graffe con l'espander

readline


Lexical analysis / tokenization: taking the input from the user and processing it char by char into “tokens”.

Syntax analysis / Parsing: scanning the stream of tokens according to a specific grammar and then deciding what to do with them (e.g. generating an AST — Abstract Syntax Tree).
!!!Abstract Syntax Tree).


 The Recursive Descent - The Theory of Computation,

  CFG (Context Free Grammar).
  
  for parsing we need to do:

1. tokenization (lexical analysis) 
	here we transform the input line into tokens
		which means that we split the input by whitespaces : (ls -l /home/user) becomes
																- ls
																- l
																- /home/user
2. syntax analysis (parsing)
	-we generate an abstract syntax tree (AST) which means that the program tries to understand what to do with 	tokes
